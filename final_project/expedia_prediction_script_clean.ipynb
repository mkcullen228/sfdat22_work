{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from datetime import date\n",
    "\n",
    "# import modelling from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Import Data and Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From Kaggle Competition. Expedia Hotel Recommendations\n",
    "# https://www.kaggle.com/c/expedia-hotel-recommendations\n",
    "\n",
    "## Load the data from expedia kaggle Challenge\n",
    "destinations = pd.read_csv('../Final_Project/data/destinations.csv') \n",
    "# Read data into pandas and explore\n",
    "expedia = pd.read_csv('../Final_Project/data/train.csv') # takes a LONG time to load the full dataset! \n",
    "# expedia = pd.read_csv('../Final_Project/data/train.csv',nrows=10000) # test code with smaller set first for faster code check. \n",
    "# other dataset provided: testing dataset\n",
    "# df_test = pd.read_csv('../Final_Project/data/test.csv'); # will read this is later when/if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expedia Site Dataframe Shape\n",
      "(37670293, 24)\n",
      "Destinations Dataframe Shape\n",
      "(62106, 150)\n"
     ]
    }
   ],
   "source": [
    "print 'Expedia Site Dataframe Shape'\n",
    "print expedia.shape\n",
    "# print type(expedia)\n",
    "# expedia.head(2)\n",
    "print 'Destinations Dataframe Shape'\n",
    "print destinations.shape\n",
    "# destinations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expedia Training when isbooking = 1\n",
      "(3000693, 24)\n"
     ]
    }
   ],
   "source": [
    "# Select df only when the user is booking the hotel. \n",
    "# only look at when a person is booking. \n",
    "\n",
    "book = expedia['is_booking'] == 1\n",
    "# Select all cases where stark is the attacker and the attacker wins\n",
    "expedia = expedia[book]\n",
    "\n",
    "# expedia = expedia[expedia.is_booking == 1]\n",
    "# expedia = expedia.loc[expedia.is_booking == 1]\n",
    "print 'Expedia Training when isbooking = 1'\n",
    "print expedia.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination of Columns from https://www.kaggle.com/c/expedia-hotel-recommendations/data \n",
    "\n",
    "date_time - Timestamp - string\n",
    "site_name - ID of the Expedia point of sale - int\n",
    "posa_continent - ID of continent associated with site_name\t- int\n",
    "user_location_country - The ID of the country the customer is located - int\n",
    "user_location_region - The ID of the region the customer is located - int\n",
    "user_location_city - The ID of the city the customer is located - int\n",
    "orig_destination_distance - Physical distance between a hotel and a customer at the time of search. A null means the distance could not be calculated- double\n",
    "user_id - ID of user - int\n",
    "is_mobile - 1 when a user connected from a mobile device, 0 otherwise - tinyint\n",
    "is_package - 1 if the click/booking was generated as a part of a package, 0 otherwise - int\n",
    "channel  - \tID of a marketing channel\n",
    "srch_ci - Checkin date - string\n",
    "srch_co - Checkout date - string\n",
    "srch_adults_cnt - The number of adults specified in the hotel room - int\n",
    "srch_children_cnt - The number of (extra occupancy) children specified in the hotel room - int\n",
    "srch_rm_cnt - The number of hotel rooms specified in the search - int\n",
    "srch_destination_id - ID of the destination where the hotel search was performed - int\n",
    "srch_destination_type_id - Type of destination - int\n",
    "hotel_continent - Hotel continent - int\n",
    "hotel_country - Hotel country - int\n",
    "hotel_market - Hotel market - int\n",
    "is_booking - 1 if a booking, 0 if a click - tinyint\n",
    "cnt - Numer of similar events in the context of the same user session  -bigint\n",
    "hotel_cluster - ID of a hotel cluster - int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Size of Dataframe (3000693, 24)\n",
      "Unique Users:  813985\n",
      "Mean entries per user:  3.68642296848\n",
      "Number of entries where users book 3000693 total 3000693\n",
      "Unique Hotel Cluter IDs:  100\n"
     ]
    }
   ],
   "source": [
    "# Print some info about dataset\n",
    "\n",
    "# user IDs. \n",
    "print 'Dataset Stats'\n",
    "print 'Size of Dataframe', expedia.shape\n",
    "\n",
    "# unique User counts. \n",
    "user = expedia.groupby('user_id').user_id.count()\n",
    "muser = user.mean()\n",
    "print 'Unique Users: ', len(user)\n",
    "print 'Mean entries per user: ', muser\n",
    "\n",
    "# Number of bookings vs. other entries. \n",
    "lbook = len(expedia.loc[(expedia['is_booking']  == 1)])\n",
    "num_book = expedia.groupby('is_booking').is_booking.count()\n",
    "print 'Number of entries where users book', lbook, 'total', len(expedia)\n",
    "\n",
    "# unique hotel cluster counts. \n",
    "hc = expedia.groupby('hotel_cluster').hotel_cluster.count()\n",
    "print 'Unique Hotel Cluter IDs: ', len(hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep Expedia Train Dataset \n",
    "# Add a column for the epoch time to dataset. \n",
    "# Need to do this before time is changed from a string to a date time\n",
    "import time, os\n",
    "# search time epoch\n",
    "epochs = []\n",
    "d = expedia.date_time\n",
    "p='%Y-%m-%d %H:%M:%S'\n",
    "for dts in d:\n",
    "    epoch = int(time.mktime(time.strptime(dts,p)))\n",
    "    epochs.append(epoch)\n",
    "expedia['search_time_epoch'] = epochs\n",
    "\n",
    "# time on site column\n",
    "# using time the user first searched as t0. i.e. 0 time = first search\n",
    "expedia['tos'] = expedia.search_time_epoch\n",
    "user = expedia.user_id\n",
    "for u in user: \n",
    "    t0 = min(expedia.loc[expedia.user_id == u].search_time_epoch)\n",
    "    expedia.loc[expedia['user_id']==u,'tos'] = expedia.search_time_epoch - t0\n",
    "\n",
    "# check-in time epoch\n",
    "# Do not have hh:mm:ss for check in and out. \n",
    "# Using 4:00 pm as default for checkout\n",
    "# epochs = []\n",
    "# d = expedia.srch_ci\n",
    "# p='%Y-%m-%d %H:%M:%S'\n",
    "# for dts in d:\n",
    "#    epoch = int(time.mktime(time.strptime(dts,p)))\n",
    "#    epochs.append(epoch)\n",
    "# expedia['srch_ci_epoch'] = epochs\n",
    "\n",
    "# check-out time epoch\n",
    "# Using 11:59 am as default for checkout\n",
    "# epochs = []\n",
    "# d = expedia.srch_co\n",
    "# p='%Y-%m-%d %H:%M:%S'\n",
    "# for dts in d:\n",
    "#    epoch = int(time.mktime(time.strptime(dts,p)))\n",
    "#    epochs.append(epoch)\n",
    "# expedia['srch_co_epoch'] = epochs\n",
    "\n",
    "expedia.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define function to find the day of the week. \n",
    "# Monday = 1, Sunday = 7\n",
    "def find_dow(date_col):\n",
    "    dow = []\n",
    "    for d in date_col: \n",
    "        dow.append(d.isoweekday()) \n",
    "    return dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding Time Feature columns to Dataset \n",
    "\n",
    "# Replacing NaN/None values in check in and out search. \n",
    "expedia['srch_ci'] = np.where(pd.isnull(expedia.srch_ci) == 1, 0, expedia.srch_ci)\n",
    "expedia['srch_co'] = np.where(pd.isnull(expedia.srch_co) == 1, 0, expedia.srch_co)\n",
    "\n",
    "expedia['date_time'] = pd.to_datetime(expedia['date_time'])\n",
    "expedia['year'] = expedia['date_time'].dt.year\n",
    "expedia['month'] = expedia['date_time'].dt.month\n",
    "expedia['day'] = expedia['date_time'].dt.day\n",
    "\n",
    "# -------------------\n",
    "# Adding Day of the week the search occured\n",
    "# Monday = 1, Sunday = 7\n",
    "date_time = expedia.date_time\n",
    "dow = find_dow(date_time)\n",
    "expedia['dow_search'] = dow\n",
    "\n",
    "# -------------------\n",
    "# Adding Day of the week check-in date\n",
    "expedia['srch_ci'] = pd.to_datetime(expedia['srch_ci'])\n",
    "date_ci = expedia.srch_ci\n",
    "dow = find_dow(date_ci)\n",
    "expedia['dow_ci'] = dow\n",
    "\n",
    "# -------------------\n",
    "# Adding Day of the week check-out date\n",
    "expedia['srch_co'] = pd.to_datetime(expedia['srch_co'])\n",
    "date_co = expedia.srch_co\n",
    "dow = find_dow(date_co)\n",
    "expedia['dow_co'] = dow\n",
    "\n",
    "# -------------------\n",
    "# length of stay in days\n",
    "delta = date_co - date_ci\n",
    "los = np.zeros(len(delta))\n",
    "c = 0\n",
    "for d in delta:\n",
    "    los[c] = d.days\n",
    "    c += 1    \n",
    "expedia['length_of_stay'] = los\n",
    "\n",
    "# -------------------\n",
    "# Adding a feature that guesses if it is a  business_trip. \n",
    "# definition: checkin Sunday(7) - Thursday(4) and must check out same week Monday(!) - Friday(5). \n",
    "# 1 = business trip, 0 = no business trip. \n",
    "\n",
    "# los = expedia.length_of_stay\n",
    "# dow_ci = expedia.dow_ci\n",
    "# dow_co = expedia.dow_co\n",
    "# bt = np.zeros(len(dow_ci))\n",
    "# c = 0;\n",
    "# for ci in dow_ci:\n",
    "#    # check if in sunday - Check that check out is Monday - Friday of the same week\n",
    "#    if ci == 7:\n",
    "#        if los[c] < 5 and dow_co[c] < 6:\n",
    "#            bt[c] = 1\n",
    "#    elif ci < 4:\n",
    "#         if los[c] < 5 and dow_co[c] <= 5:\n",
    "#            bt[c] = 1\n",
    "#    c += 1\n",
    "# expedia['is_business_trip'] = bt \n",
    "\n",
    "# expedia.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " feature ideas: \n",
    " number or clicks before purchase\n",
    " repeat clicks? user visits same hotel cluster again \n",
    " \n",
    " if user books more than one hotel - is it in the same cluster, same location? \n",
    " time on site searching column (from time_epoch)\n",
    " \n",
    " add dictionary look up for user.\n",
    " Recommendation engine? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding feature columns to dataset. \n",
    "\n",
    "# if there are children - making a binary column of family\n",
    "is_family = np.where(expedia.srch_children_cnt >= 1, 1, 0)\n",
    "expedia['is_family'] = is_family\n",
    "\n",
    "# Adding column for search and is booking. \n",
    "# booking cluster  = 100 + cluster ID if booked,  0 if not booking\n",
    "expedia['booking_clust'] = np.where(expedia.is_booking == 1,(100+ expedia.hotel_cluster) ,0)\n",
    "\n",
    "# expedia.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prep Destinations Dataset\n",
    "# Adding Mean, Meadian, stdev, is_positive columns \n",
    "d = destinations\n",
    "d = d.drop('srch_destination_id',axis = 1)\n",
    "\n",
    "destinations['mean_latent'] = np.nanmean(d, axis=1)\n",
    "destinations['median_latent'] = np.nanmedian(d, axis=1)\n",
    "destinations['std_latent'] = np.nanstd(destinations, axis=1)\n",
    "destinations['mean_p_std_latent'] = abs(destinations.mean_latent) + destinations.std_latent\n",
    "destinations['max_latent'] = np.nanmax(d, axis=1)\n",
    "destinations['min_latent'] = np.nanmin(d, axis=1)\n",
    "destinations['is_positive_review'] = np.where(destinations.mean_latent > 0, 1, 0)\n",
    "destinations['range_latent'] = destinations.max_latent - destinations.min_latent\n",
    "\n",
    "# use only summary columns in training. \n",
    "use = ['srch_destination_id', 'mean_latent', 'median_latent', 'std_latent',\\\n",
    "       'mean_p_std_latent', 'max_latent', 'min_latent','is_positive_review',\\\n",
    "       'range_latent']\n",
    "destinations_use =  destinations[use]\n",
    "\n",
    "# print destinations.shape\n",
    "# print destinations_use.shape\n",
    "# print destinations_use.isnull().sum()\n",
    "# destinations_use.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add Destination Data Information to Expedia Dataset\n",
    "# Adding by search destination ID\n",
    "\n",
    "# print expedia.shape\n",
    "# print destinations_use.shape\n",
    "expedia = pd.merge(expedia, destinations_use, left_on='srch_destination_id', \\\n",
    "                   right_on='srch_destination_id', how='left')\n",
    "\n",
    "# print expedia.shape\n",
    "# expedia.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix NaN and Null values\n",
    "expedia['mean_latent'] = np.where(pd.isnull(expedia.mean_latent) == 1, 0, expedia.mean_latent)\n",
    "expedia['median_latent'] = np.where(pd.isnull(expedia.median_latent) == 1, 0, expedia.median_latent)\n",
    "expedia['std_latent'] = np.where(pd.isnull(expedia.std_latent) == 1, 0, expedia.std_latent)\n",
    "expedia['mean_p_std_latent'] = np.where(pd.isnull(expedia.mean_p_std_latent) == 1, 0, expedia.mean_p_std_latent)\n",
    "expedia['max_latent'] = np.where(pd.isnull(expedia.max_latent) == 1, 0, expedia.max_latent)\n",
    "expedia['min_latent'] = np.where(pd.isnull(expedia.min_latent) == 1, 0, expedia.min_latent)\n",
    "expedia['is_positive_review'] = np.where(pd.isnull(expedia.is_positive_review) == 1, 0, expedia.is_positive_review)\n",
    "expedia['range_latent'] = np.where(pd.isnull(expedia.range_latent) == 1, 0, expedia.range_latent)\n",
    "\n",
    "# some features have negative values. making abs(features) for NB  \n",
    "expedia['abs_mean_latent'] = expedia['mean_latent'].abs()\n",
    "expedia['abs_median_latent'] = expedia['median_latent'].abs()\n",
    "expedia['abs_max_latent'] = expedia['max_latent'].abs()\n",
    "expedia['abs_min_latent'] = expedia['min_latent'].abs()\n",
    "\n",
    "print expedia.shape\n",
    "expedia.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print expedia.isnull().sum()\n",
    "# print expedia.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Predicting Which Hotel Cluseter user will book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Target and Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For predicting which hotel cluster the person will book. \n",
    "features = ['site_name', 'posa_continent', 'user_location_country',\\\n",
    "            'user_location_region', 'user_location_city',\\\n",
    "            'user_id', 'is_mobile', 'is_package',\\\n",
    "            'channel', 'srch_adults_cnt',\\\n",
    "            'srch_children_cnt', 'srch_rm_cnt', 'srch_destination_id',\\\n",
    "            'srch_destination_type_id', 'cnt', 'hotel_continent',\\\n",
    "            'hotel_country', 'hotel_market', 'hotel_cluster',\\\n",
    "            'search_time_epoch', 'tos', 'year', 'month', 'day', 'dow_search',\\\n",
    "            'dow_ci', 'dow_co', 'length_of_stay',\\\n",
    "            'is_family', 'mean_latent', 'median_latent',\\\n",
    "            'std_latent', 'mean_p_std_latent', 'max_latent',\\\n",
    "            'min_latent', 'is_positive_review', 'range_latent',\\\n",
    "            'abs_mean_latent','abs_median_latent', 'abs_max_latent','abs_min_latent']\n",
    "\n",
    "# 'is_business_trip',\n",
    "df_book = expedia\n",
    "print 'Traing dataset', df_book.shape\n",
    "\n",
    "# df_book.corr()\n",
    "ignore = ['hotel_cluster', 'date_time', 'srch_ci', 'srch_co',\\\n",
    "          'booking_clust', 'orig_destination_distance']\n",
    "X = df_book.drop(ignore, axis = 1) # features\n",
    "target = df_book.hotel_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Null model\n",
    "# Compare your best RMSE on testing set with the RMSE for the \"null model\", which is the model that ignores\n",
    "#   all features and simply predicts the mean rating in the training set for all observations in the testing set.\n",
    "\n",
    "# Cross-Validation by\n",
    "cvn = 10\n",
    "\n",
    "predv = (df_book.hotel_cluster.mode()[0])\n",
    "null_pred = np.ones(len(df_book.hotel_cluster)) * predv\n",
    "df_book['prediction'] = null_pred\n",
    "# y_train['prediction'] = np.ones(len(y_train.hotel_cluster)) * predv\n",
    "# print df_book.isnull().sum()\n",
    "\n",
    "# calculate RMSE for those predictions\n",
    "rmse_null = np.sqrt(metrics.mean_squared_error(df_book.hotel_cluster, df_book.prediction))\n",
    "\n",
    "# rmse_null_test = np.sqrt(metrics.mean_squared_error(y_test.hotel_cluster, y_train.prediction))\n",
    "print 'Hypthesis Value: ', predv \n",
    "print 'Null Hypothesis RMSE: ', rmse_null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find best K to use Grid Search 2:50 by 1\n",
    "k = 3\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "k_range = range(2, 50, 1)\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "grid.fit(X, target)\n",
    "grid.grid_scores_\n",
    "grid_mean_scores = [result[1] for result in grid.grid_scores_] # get scores from grid.grid_scores_\n",
    "# visualize results\n",
    "plt.figure()\n",
    "plt.plot(k_range, grid_mean_scores)\n",
    "\n",
    "print 'best grid score', grid.best_score_ \n",
    "best_k = grid.best_params_['n_neighbors']\n",
    "\n",
    "print 'Using K = ', best_k\n",
    "# knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X, target)\n",
    "knn.score(X, target)\n",
    "preds_knn = knn.predict(X)\n",
    "knn_model = grid.best_estimator_ \n",
    "\n",
    "print 'Accuracy for KNN model'\n",
    "print accuracy_score(target, preds_knn)\n",
    "# print 'KNN Confusion Matrix'\n",
    "# print metrics.confusion_matrix(target, preds_knn)\n",
    "rmse_knn = np.sqrt(metrics.mean_squared_error(target, preds_knn)) \n",
    "print 'KNN RMSE', rmse_knn\n",
    "\n",
    "scores = cross_val_score(knn, X, target, cv=cvn, scoring='mean_squared_error')\n",
    "rmse_knn_cv = np.mean(np.sqrt(-scores))\n",
    "print 'KNN RMSE CV', rmse_knn_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X, target)\n",
    "assorted_pred_class = logreg.predict(X)\n",
    "assorted_pred_prob = logreg.predict_proba(X)[:, 1]\n",
    "intercept = logreg.intercept_\n",
    "logregm = zip(X,logreg.coef_[0]) # examine coeff\n",
    "preds_logreg = logreg.predict(X)\n",
    "\n",
    "logodds = logreg.intercept_\n",
    "odds = np.exp(logodds)\n",
    "prob = odds/(1 + odds)\n",
    "prob\n",
    "\n",
    "print 'Accuracy for Logistic Regression model'\n",
    "print accuracy_score(target, preds_logreg)\n",
    "# print 'Logistic Regression Confusion Matrix'\n",
    "# print metrics.confusion_matrix(target, preds_logreg)\n",
    "# print(classification_report(target, preds_logreg))\n",
    "rmse_logreg = np.sqrt(metrics.mean_squared_error(target, preds_logreg)) \n",
    "print 'Log Reg RMSE', rmse_logreg\n",
    "\n",
    "scores = cross_val_score(logreg, X, target, cv=cvn, scoring='mean_squared_error')\n",
    "rmse_logreg_cv = np.mean(np.sqrt(-scores))\n",
    "print 'Log Reg RMSE CV', rmse_logreg_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Niave Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignoring negative channels for nb. \n",
    "ignore_nb = ['hotel_cluster', 'date_time', 'srch_ci', 'srch_co',\\\n",
    "             'booking_clust', 'orig_destination_distance',\\\n",
    "             'mean_latent','median_latent', 'max_latent','min_latent']\n",
    "X_nb = df_book.drop(ignore_nb, axis = 1) # features\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_nb, target)\n",
    "\n",
    "# three paramaters you can set: \n",
    "# alpha=1.0, \n",
    "# class_prior=None, \n",
    "# fit_prior=True - estimates the likelihood function - counting the probability for each. \n",
    "\n",
    "preds_nb = nb.predict(X_nb)\n",
    "print metrics.accuracy_score(target, preds_nb)\n",
    "# print metrics.confusion_matrix(target, preds_nb)\n",
    "\n",
    "rmse_nb = np.sqrt(metrics.mean_squared_error(target, preds_nb)) \n",
    "print 'RMSE for NB', rmse_nb\n",
    "\n",
    "scores = cross_val_score(nb, X_nb, target, cv=cvn, scoring='mean_squared_error')\n",
    "rmse_nb_cv = np.mean(np.sqrt(-scores))\n",
    "print 'CV RMSE for NB', rmse_nb_cv\n",
    "\n",
    "# probs_nb = nb.predict_proba(X)[:, 1]\n",
    "# probs_nb\n",
    "# print metrics.roc_auc_score(target, probs_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying differnt values for Alpha \n",
    "# three paramaters you can set: \n",
    "# alpha=1.0, \n",
    "# class_prior=None, \n",
    "# fit_prior=True - estimates the likelihood function - counting the probability for each. \n",
    "MSE_scores = []\n",
    "RMSE_scores = []\n",
    "alpha_range = range(1,30,2)\n",
    "for a in alpha_range:\n",
    "    nb = MultinomialNB(alpha = a)\n",
    "    nb.fit(X_nb, target)\n",
    "    MSE_scores = cross_val_score(nb, X_nb, target, cv=cvn, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "plt.plot(alpha_range, RMSE_scores)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "\n",
    "feature_scores = sorted(zip(RMSE_scores, alpha_range))[0]\n",
    "best_alpha = feature_scores[1]\n",
    "\n",
    "print feature_scores\n",
    "print 'Best Alpha to use', best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying differnt values for Alpha, fit_prior = false\n",
    "# three paramaters you can set: \n",
    "# alpha=1.0, \n",
    "# class_prior=None, \n",
    "# fit_prior=True - estimates the likelihood function - counting the probability for each. \n",
    "MSE_scores = []\n",
    "RMSE_scores = []\n",
    "alpha_range = range(1,30,2)\n",
    "for a in alpha_range:\n",
    "    nb = MultinomialNB(alpha = a, fit_prior = False)\n",
    "    nb.fit(X_nb, target)\n",
    "    MSE_scores = cross_val_score(nb, X_nb, target, cv=cvn, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "plt.plot(alpha_range, RMSE_scores)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "\n",
    "feature_scores = sorted(zip(RMSE_scores, alpha_range))[0]\n",
    "best_alpha = feature_scores[1]\n",
    "\n",
    "print feature_scores\n",
    "print 'Best Alpha to use', best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "# TODO grid search for depth and\n",
    "\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, target)\n",
    "preds_dtree = treeclf.predict(X)\n",
    "predic_proba = treeclf.predict_proba(X)\n",
    "sc = treeclf.score(X,target)\n",
    "\n",
    "# export_graphviz(treeclf, out_file='tree_titanic.dot', feature_names=features)\n",
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_titanic.dot -o tree_titanic.png\n",
    "\n",
    "# compute the feature importances\n",
    "# importaces are on a scale on 0 to 1\n",
    "pd.DataFrame({'feature':features, 'importance':treeclf.feature_importances_})\n",
    "\n",
    "print 'Mean Accuracy on Test Data and Labels', sc\n",
    "rmse_dtree = np.sqrt(metrics.mean_squared_error(target, preds_dtree))\n",
    "print 'RMSE DTREE', rmse_dtree\n",
    "\n",
    "scores = cross_val_score(treeclf, X, target, cv=cvn, scoring='mean_squared_error')\n",
    "rmse_dtree_cv = np.mean(np.sqrt(-scores))\n",
    "print 'CV RMSE DTREE', rmse_dtree_cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Search For better settings for Decision Tree\n",
    "# searching for max_depth and min_samples_leaf\n",
    "\n",
    "# search max_depth\n",
    "max_depth_range = range(1,30,1)\n",
    "MSE_scores = []\n",
    "RMSE_scores= []\n",
    "for md in max_depth_range: \n",
    "    treeclf = DecisionTreeClassifier(max_depth=md, random_state=1)\n",
    "    treeclf.fit(X, target)\n",
    "    MSE_scores = cross_val_score(treeclf, X, target, cv=cvn, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "plt.plot(max_depth_range, RMSE_scores)\n",
    "plt.xlabel('Max Depth Allowed')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "\n",
    "depth_scores = sorted(zip(RMSE_scores, max_depth_range))[0]\n",
    "best_depth = depth_scores[1]\n",
    "print 'Best Max Depth Score', depth_scores\n",
    "print 'Best Max Depth to use', best_depth\n",
    "\n",
    "\n",
    "# search min_samples_leaf\n",
    "min_samples_leaf_range = range(1,215,5)\n",
    "MSE_scores = []\n",
    "RMSE_scores= []\n",
    "for msl in min_samples_leaf_range: \n",
    "    treeclf = DecisionTreeClassifier(min_samples_leaf=msl, random_state=1)\n",
    "    treeclf.fit(X, target)\n",
    "    MSE_scores = cross_val_score(treeclf, X, target, cv=cvn, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "\n",
    "plt.plot(min_samples_leaf_range, RMSE_scores)\n",
    "plt.xlabel('Max Depth Allowed')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "\n",
    "min_samp_scores = sorted(zip(RMSE_scores, min_samples_leaf_range))[0]\n",
    "best_min_samp = min_samp_scores[1]\n",
    "print 'Best Min Samples Score', min_samp_scores\n",
    "print 'Best Min Samples to use', best_min_samp\n",
    "\n",
    "\n",
    "# combine best settings discovered in two separate searches \n",
    "treeclf = DecisionTreeClassifier(max_depth=best_depth, min_samples_leaf = best_min_samp, random_state=1)\n",
    "treeclf.fit(X, target)\n",
    "preds_dtree = treeclf.predict(X)\n",
    "# predic_proba = treeclf.predict_proba(X)\n",
    "sc = treeclf.score(X,target)\n",
    "\n",
    "# compute the feature importances\n",
    "# importaces are on a scale on 0 to 1\n",
    "pd.DataFrame({'feature':features, 'importance':treeclf.feature_importances_})\n",
    "\n",
    "print 'Mean Accuracy on Test Data and Labels', sc\n",
    "rmse_dtree_best_set = np.sqrt(metrics.mean_squared_error(target, preds_dtree))\n",
    "print 'RMSE DTREE Best Set', rmse_dtree_best_set\n",
    "\n",
    "scores = cross_val_score(treeclf, X, target, cv=cvn, scoring='mean_squared_error')\n",
    "rmse_dtree_best_set_cv = np.mean(np.sqrt(-scores))\n",
    "print 'CV RMSE DTREE Best Set', rmse_dtree_best_set_cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "rfreg = RandomForestClassifier()\n",
    "\n",
    "# Tuning n_estimators\n",
    "# list of values to try for n_estimators\n",
    "estimator_range = range(10, 310, 10)\n",
    "\n",
    "# list to store the average RMSE for each value of n_estimators\n",
    "RMSE_scores = []\n",
    "\n",
    "# use 5-fold cross-validation with each value of n_estimators (WARNING: SLOW!)\n",
    "for estimator in estimator_range:\n",
    "    rfreg = RandomForestClassifier(n_estimators=estimator, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, target, cv=5, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "    \n",
    "plt.plot(estimator_range, RMSE_scores)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_n = 175\n",
    "# Tuning max Features\n",
    "# list of values to try for max_features\n",
    "feature_range = range(1, len(features)+1)\n",
    "\n",
    "# list to store the average RMSE for each value of max_features\n",
    "RMSE_scores = []\n",
    "\n",
    "# use 10-fold cross-validation with each value of max_features (WARNING: SLOW!)\n",
    "for feature in feature_range:\n",
    "    rfreg = RandomForestClassifier(n_estimators=best_n, max_features=feature, random_state=1)\n",
    "    MSE_scores = cross_val_score(rfreg, X, target, cv=10, scoring='mean_squared_error')\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "    \n",
    "plt.plot(feature_range, RMSE_scores)\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('RMSE (lower is better)')\n",
    "\n",
    "feature_scores = sorted(zip(RMSE_scores, feature_range))[0]\n",
    "best_max_feat = feature_scores[1]\n",
    "\n",
    "print feature_scores\n",
    "print 'Best Max Features to use', best_max_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fitting Random Forest with best parameters\n",
    "# max_features=8 is best and n_estimators=150 is sufficiently large\n",
    "rfclass = RandomForestClassifier(n_estimators = best_n, max_features = best_max_feat, oob_score=True, random_state=1)\n",
    "rfclass.fit(X, target)\n",
    "\n",
    "# compute feature importances\n",
    "pd.DataFrame({'feature':features, 'importance':rfclass.feature_importances_}).sort('importance')\n",
    "\n",
    "# compute the out-of-bag R-squared score\n",
    "rfclass.oob_score_\n",
    "preds_rf = rfclass.predict(X)\n",
    "rmse_rf = np.sqrt(metrics.mean_squared_error(target, preds_rf))\n",
    "print 'RMSE RF', rmse_rf\n",
    "\n",
    "scores = cross_val_score(rfclass, X, target, cv=10, scoring='mean_squared_error')\n",
    "rmse_rf_cv = np.mean(np.sqrt(-scores))\n",
    "print 'CV RMSE RF', rmse_rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'RESULTS and SCORES SUMMARY'\n",
    "print 'Null Hypothesis RMSE', rmse_null\n",
    "print ''\n",
    "print 'KNN RMSE', rmse_knn\n",
    "print 'Log Reg RMSE', rmse_logreg\n",
    "print 'NB RMSE', rmse_nb\n",
    "print 'DTREE RMSE', rmse_dtree\n",
    "print 'Random Forest RMSE', rmse_rf\n",
    "# print 'Random Forest Important Features RMSE', rmse_rf_imp\n",
    "\n",
    "print ''\n",
    "print 'CV KNN RMSE', rmse_knn_cv\n",
    "print 'CV Log Reg RMSE', rmse_logreg_cv\n",
    "print 'CV NB RMSE', rmse_nb_cv\n",
    "print 'CV DTREE RMSE', rmse_dtree_cv\n",
    "print 'CV Random Forest RMSE', rmse_rf_cv\n",
    "# print 'CV Random Forest Important Features RMSE', rmse_rf_imp_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final Test Full model With completely unseen test data. \n",
    "# combination of best first classification + predicting hotel cluster. \n",
    "# df_test = pd.read_csv('../Final_Project/data/test.csv'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
